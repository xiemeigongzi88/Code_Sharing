{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14c160ae80afea8c7cc27f29fd2942af",
   "metadata": {
    "colab_type": "text",
    "id": "woHnuhYHCNn9"
   },
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897b75a6",
   "metadata": {},
   "source": [
    "<font color='red'>Write your name below:</font>\n",
    "\n",
    "Shuang Wang "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8539708",
   "metadata": {},
   "source": [
    "In this homework, we will train a CNN model on CIFAR-10.\n",
    "The rest of this notebook contains a template to build a CNN to classify [CIFAR-10](https://www.cs.toronto.edu/%7Ekriz/cifar.html).\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. \n",
    "Below are several images from CIFAR-10:\n",
    "\n",
    "![](https://www.tensorflow.org/tutorials/images/cnn_files/output_K3PAELE2eSU9_0.png )\n",
    "\n",
    "*Your jobs*\n",
    "1. Read and understand the structure of porvided code.\n",
    "2. Fill in the missing code block.\n",
    "3. Tune the hyper-parameters to maximize the accurcy.\n",
    "4. Execute the whole IPYNB to obtain the results.\n",
    "    - Export the IPYNB file with results as a HTML.\n",
    "    - Zip and submit IPYNB and HTML files to Canvas.\n",
    "    - Missing the output of execution may hurt your grade.\n",
    "\n",
    "You can find the models with state-of-the art performance on CIFAR-10 [here](https://paperswithcode.com/sota/image-classification-on-cifar-10)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95ff04c",
   "metadata": {},
   "source": [
    "## Hints to Improve Your Results\n",
    "\n",
    "1. Start from the simple Softmax model.\n",
    "2. Add Conv and activation layers\n",
    "3. Tune hyper-parameters, such as batch_size, learning rate, optimizer\n",
    "4. Try different CNN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c32f238",
   "metadata": {},
   "source": [
    "First, import the packages or modules required for the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69d8858e",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    },
    "colab": {},
    "colab_type": "code",
    "id": "K_4CLE8YCNn_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import random\n",
    "\n",
    "torch.manual_seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64f86c2-b89a-4f31-8b2a-a757de007516",
   "metadata": {},
   "source": [
    "## Define a few hyper parameteers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dfdf238-d4c4-48ea-a12a-b6d2e8418c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to tune the parameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "epoch = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bd51d2",
   "metadata": {
    "colab_type": "text",
    "id": "KCMc2Qo2IF0F"
   },
   "source": [
    "### Loading and normalizing \n",
    "\n",
    "Using torchvision, itâ€™s extremely easy to load CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c39f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c25af9f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4-1q71QrGj3g",
    "outputId": "d1c278cd-a268-46c4-ce9e-6c9af9bb21ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    # Feel free to tune the transform\n",
    "    transforms.RandomHorizontalFlip(p=.40),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "# split into training and validation set\n",
    "trainset, valset = random_split(trainset, [42000,8000])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, \n",
    "                                        shuffle=False, num_workers=2)\n",
    "# load testing set\n",
    "testset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=test_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c81f5e",
   "metadata": {
    "colab_type": "text",
    "id": "F8WhH_4KCNo3"
   },
   "source": [
    "## Define the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43be4554",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LlWELujJY9ks"
   },
   "outputs": [],
   "source": [
    "############## Add Your Code Here ##########################\n",
    "\n",
    "net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10))\n",
    "\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d24090-8038-47d5-ae39-7fb63b3c3cee",
   "metadata": {},
   "source": [
    "## Define your loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80b4660e-97b1-4f09-a53b-3fd56f9ecbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Add Your Code Here ##########################\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ad2954-7883-41e8-9562-c3757b956896",
   "metadata": {},
   "source": [
    "## Define your optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a60e40d5-9eeb-4b65-985b-0b0327912d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Add Your Code Here ##########################\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4455fe7a",
   "metadata": {
    "colab_type": "text",
    "id": "_YsKgVvWCNpC"
   },
   "source": [
    "## Define the Training Procedure\n",
    "\n",
    "We will select the model and tune hyper-parameters according to the model's performance on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49f725d9",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "JyoNJsjFCNpG",
    "outputId": "064ad920-6176-429a-d3dd-de7845687896"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy at Epoch   1:\t\t36.1048%\n",
      "Validation accuracy at Epoch   1:\t47.2375%\n",
      "Train accuracy at Epoch   2:\t\t52.6905%\n",
      "Validation accuracy at Epoch   2:\t56.1625%\n",
      "Train accuracy at Epoch   3:\t\t59.1452%\n",
      "Validation accuracy at Epoch   3:\t60.5625%\n",
      "Train accuracy at Epoch   4:\t\t63.6310%\n",
      "Validation accuracy at Epoch   4:\t63.4750%\n",
      "Train accuracy at Epoch   5:\t\t66.0357%\n",
      "Validation accuracy at Epoch   5:\t64.6000%\n",
      "Train accuracy at Epoch   6:\t\t68.3476%\n",
      "Validation accuracy at Epoch   6:\t67.8250%\n",
      "Train accuracy at Epoch   7:\t\t69.8833%\n",
      "Validation accuracy at Epoch   7:\t68.7500%\n",
      "Train accuracy at Epoch   8:\t\t71.0571%\n",
      "Validation accuracy at Epoch   8:\t69.3625%\n",
      "Train accuracy at Epoch   9:\t\t71.8929%\n",
      "Validation accuracy at Epoch   9:\t69.7500%\n",
      "Train accuracy at Epoch  10:\t\t72.7548%\n",
      "Validation accuracy at Epoch  10:\t71.0625%\n",
      "Train accuracy at Epoch  11:\t\t73.8429%\n",
      "Validation accuracy at Epoch  11:\t71.1750%\n",
      "Train accuracy at Epoch  12:\t\t74.6167%\n",
      "Validation accuracy at Epoch  12:\t71.4875%\n",
      "Train accuracy at Epoch  13:\t\t75.4167%\n",
      "Validation accuracy at Epoch  13:\t72.6125%\n",
      "Train accuracy at Epoch  14:\t\t75.4548%\n",
      "Validation accuracy at Epoch  14:\t72.6375%\n",
      "Train accuracy at Epoch  15:\t\t75.9095%\n",
      "Validation accuracy at Epoch  15:\t72.7000%\n",
      "Train accuracy at Epoch  16:\t\t76.7167%\n",
      "Validation accuracy at Epoch  16:\t74.1500%\n",
      "Train accuracy at Epoch  17:\t\t76.9143%\n",
      "Validation accuracy at Epoch  17:\t73.6625%\n",
      "Train accuracy at Epoch  18:\t\t77.7571%\n",
      "Validation accuracy at Epoch  18:\t72.6250%\n",
      "Train accuracy at Epoch  19:\t\t77.7714%\n",
      "Validation accuracy at Epoch  19:\t73.4375%\n",
      "Train accuracy at Epoch  20:\t\t77.8024%\n",
      "Validation accuracy at Epoch  20:\t73.6625%\n",
      "Train accuracy at Epoch  21:\t\t78.1762%\n",
      "Validation accuracy at Epoch  21:\t74.0375%\n",
      "Train accuracy at Epoch  22:\t\t78.6833%\n",
      "Validation accuracy at Epoch  22:\t74.7000%\n",
      "Train accuracy at Epoch  23:\t\t79.0714%\n",
      "Validation accuracy at Epoch  23:\t74.2375%\n",
      "Train accuracy at Epoch  24:\t\t79.4738%\n",
      "Validation accuracy at Epoch  24:\t74.0375%\n",
      "Train accuracy at Epoch  25:\t\t79.5405%\n",
      "Validation accuracy at Epoch  25:\t75.0750%\n",
      "Train accuracy at Epoch  26:\t\t79.9786%\n",
      "Validation accuracy at Epoch  26:\t75.8500%\n",
      "Train accuracy at Epoch  27:\t\t79.8214%\n",
      "Validation accuracy at Epoch  27:\t74.4625%\n",
      "Train accuracy at Epoch  28:\t\t80.2071%\n",
      "Validation accuracy at Epoch  28:\t75.7000%\n",
      "Train accuracy at Epoch  29:\t\t80.2190%\n",
      "Validation accuracy at Epoch  29:\t75.0125%\n",
      "Train accuracy at Epoch  30:\t\t80.5833%\n",
      "Validation accuracy at Epoch  30:\t75.4875%\n",
      "Train accuracy at Epoch  31:\t\t80.6762%\n",
      "Validation accuracy at Epoch  31:\t75.1125%\n",
      "Train accuracy at Epoch  32:\t\t80.9762%\n",
      "Validation accuracy at Epoch  32:\t74.8375%\n",
      "Train accuracy at Epoch  33:\t\t81.0405%\n",
      "Validation accuracy at Epoch  33:\t75.9750%\n",
      "Train accuracy at Epoch  34:\t\t81.3119%\n",
      "Validation accuracy at Epoch  34:\t76.9500%\n",
      "Train accuracy at Epoch  35:\t\t81.3357%\n",
      "Validation accuracy at Epoch  35:\t76.2625%\n",
      "Train accuracy at Epoch  36:\t\t81.9310%\n",
      "Validation accuracy at Epoch  36:\t77.1750%\n",
      "Train accuracy at Epoch  37:\t\t82.1167%\n",
      "Validation accuracy at Epoch  37:\t75.5375%\n",
      "Train accuracy at Epoch  38:\t\t82.2714%\n",
      "Validation accuracy at Epoch  38:\t75.9500%\n",
      "Train accuracy at Epoch  39:\t\t82.4167%\n",
      "Validation accuracy at Epoch  39:\t75.5375%\n",
      "Train accuracy at Epoch  40:\t\t82.3500%\n",
      "Validation accuracy at Epoch  40:\t75.6500%\n",
      "Train accuracy at Epoch  41:\t\t82.2976%\n",
      "Validation accuracy at Epoch  41:\t77.6250%\n",
      "Train accuracy at Epoch  42:\t\t82.3452%\n",
      "Validation accuracy at Epoch  42:\t74.5875%\n",
      "Train accuracy at Epoch  43:\t\t82.8214%\n",
      "Validation accuracy at Epoch  43:\t76.8375%\n",
      "Train accuracy at Epoch  44:\t\t82.8500%\n",
      "Validation accuracy at Epoch  44:\t77.1625%\n",
      "Train accuracy at Epoch  45:\t\t83.4619%\n",
      "Validation accuracy at Epoch  45:\t76.2500%\n",
      "Train accuracy at Epoch  46:\t\t83.2738%\n",
      "Validation accuracy at Epoch  46:\t77.2500%\n",
      "Train accuracy at Epoch  47:\t\t83.0333%\n",
      "Validation accuracy at Epoch  47:\t76.8125%\n",
      "Train accuracy at Epoch  48:\t\t83.3905%\n",
      "Validation accuracy at Epoch  48:\t76.4750%\n",
      "Train accuracy at Epoch  49:\t\t83.8286%\n",
      "Validation accuracy at Epoch  49:\t77.2625%\n",
      "Train accuracy at Epoch  50:\t\t83.5095%\n",
      "Validation accuracy at Epoch  50:\t75.8375%\n",
      "Train accuracy at Epoch  51:\t\t83.2167%\n",
      "Validation accuracy at Epoch  51:\t75.9875%\n",
      "Train accuracy at Epoch  52:\t\t83.7429%\n",
      "Validation accuracy at Epoch  52:\t76.7250%\n",
      "Train accuracy at Epoch  53:\t\t83.8929%\n",
      "Validation accuracy at Epoch  53:\t76.7125%\n",
      "Train accuracy at Epoch  54:\t\t84.2905%\n",
      "Validation accuracy at Epoch  54:\t78.4000%\n",
      "Train accuracy at Epoch  55:\t\t83.5500%\n",
      "Validation accuracy at Epoch  55:\t76.1875%\n",
      "Train accuracy at Epoch  56:\t\t84.0786%\n",
      "Validation accuracy at Epoch  56:\t77.1625%\n",
      "Train accuracy at Epoch  57:\t\t84.1667%\n",
      "Validation accuracy at Epoch  57:\t76.1750%\n",
      "Train accuracy at Epoch  58:\t\t84.3500%\n",
      "Validation accuracy at Epoch  58:\t78.1625%\n",
      "Train accuracy at Epoch  59:\t\t84.4810%\n",
      "Validation accuracy at Epoch  59:\t75.6500%\n",
      "Train accuracy at Epoch  60:\t\t84.3881%\n",
      "Validation accuracy at Epoch  60:\t76.1500%\n",
      "Train accuracy at Epoch  61:\t\t84.4071%\n",
      "Validation accuracy at Epoch  61:\t78.4250%\n",
      "Train accuracy at Epoch  62:\t\t84.3548%\n",
      "Validation accuracy at Epoch  62:\t77.1500%\n",
      "Train accuracy at Epoch  63:\t\t84.7738%\n",
      "Validation accuracy at Epoch  63:\t76.1000%\n",
      "Train accuracy at Epoch  64:\t\t84.7000%\n",
      "Validation accuracy at Epoch  64:\t77.8375%\n",
      "Train accuracy at Epoch  65:\t\t84.5548%\n",
      "Validation accuracy at Epoch  65:\t76.6625%\n",
      "Train accuracy at Epoch  66:\t\t84.9429%\n",
      "Validation accuracy at Epoch  66:\t76.7250%\n",
      "Train accuracy at Epoch  67:\t\t85.5238%\n",
      "Validation accuracy at Epoch  67:\t78.3500%\n",
      "Train accuracy at Epoch  68:\t\t85.2071%\n",
      "Validation accuracy at Epoch  68:\t77.3250%\n",
      "Train accuracy at Epoch  69:\t\t79.3381%\n",
      "Validation accuracy at Epoch  69:\t73.1750%\n",
      "Train accuracy at Epoch  70:\t\t82.9929%\n",
      "Validation accuracy at Epoch  70:\t77.6625%\n",
      "Train accuracy at Epoch  71:\t\t85.3952%\n",
      "Validation accuracy at Epoch  71:\t77.9625%\n",
      "Train accuracy at Epoch  72:\t\t85.5905%\n",
      "Validation accuracy at Epoch  72:\t75.1625%\n",
      "Train accuracy at Epoch  73:\t\t86.3357%\n",
      "Validation accuracy at Epoch  73:\t77.2250%\n",
      "Train accuracy at Epoch  74:\t\t85.7214%\n",
      "Validation accuracy at Epoch  74:\t77.3875%\n",
      "Train accuracy at Epoch  75:\t\t85.7548%\n",
      "Validation accuracy at Epoch  75:\t77.1000%\n",
      "Train accuracy at Epoch  76:\t\t85.6190%\n",
      "Validation accuracy at Epoch  76:\t75.6375%\n",
      "Train accuracy at Epoch  77:\t\t85.5143%\n",
      "Validation accuracy at Epoch  77:\t77.2500%\n",
      "Train accuracy at Epoch  78:\t\t84.6167%\n",
      "Validation accuracy at Epoch  78:\t77.2125%\n",
      "Train accuracy at Epoch  79:\t\t84.3190%\n",
      "Validation accuracy at Epoch  79:\t77.7500%\n",
      "Train accuracy at Epoch  80:\t\t86.1190%\n",
      "Validation accuracy at Epoch  80:\t76.9250%\n",
      "Train accuracy at Epoch  81:\t\t84.9452%\n",
      "Validation accuracy at Epoch  81:\t77.2375%\n",
      "Train accuracy at Epoch  82:\t\t85.3548%\n",
      "Validation accuracy at Epoch  82:\t77.6500%\n",
      "Train accuracy at Epoch  83:\t\t85.0381%\n",
      "Validation accuracy at Epoch  83:\t75.7000%\n",
      "Train accuracy at Epoch  84:\t\t84.6429%\n",
      "Validation accuracy at Epoch  84:\t75.4875%\n",
      "Train accuracy at Epoch  85:\t\t85.3810%\n",
      "Validation accuracy at Epoch  85:\t77.3250%\n",
      "Train accuracy at Epoch  86:\t\t86.3548%\n",
      "Validation accuracy at Epoch  86:\t77.9625%\n",
      "Train accuracy at Epoch  87:\t\t85.8167%\n",
      "Validation accuracy at Epoch  87:\t76.8375%\n",
      "Train accuracy at Epoch  88:\t\t87.0048%\n",
      "Validation accuracy at Epoch  88:\t76.9375%\n",
      "Train accuracy at Epoch  89:\t\t83.3119%\n",
      "Validation accuracy at Epoch  89:\t77.2000%\n",
      "Train accuracy at Epoch  90:\t\t86.0929%\n",
      "Validation accuracy at Epoch  90:\t78.1375%\n",
      "Train accuracy at Epoch  91:\t\t85.9119%\n",
      "Validation accuracy at Epoch  91:\t77.7625%\n",
      "Train accuracy at Epoch  92:\t\t85.7238%\n",
      "Validation accuracy at Epoch  92:\t77.2375%\n",
      "Train accuracy at Epoch  93:\t\t85.7548%\n",
      "Validation accuracy at Epoch  93:\t76.6125%\n",
      "Train accuracy at Epoch  94:\t\t84.2500%\n",
      "Validation accuracy at Epoch  94:\t77.7125%\n",
      "Train accuracy at Epoch  95:\t\t86.2952%\n",
      "Validation accuracy at Epoch  95:\t77.2875%\n",
      "Train accuracy at Epoch  96:\t\t87.0143%\n",
      "Validation accuracy at Epoch  96:\t78.0500%\n",
      "Train accuracy at Epoch  97:\t\t86.1929%\n",
      "Validation accuracy at Epoch  97:\t76.5250%\n",
      "Train accuracy at Epoch  98:\t\t86.1952%\n",
      "Validation accuracy at Epoch  98:\t78.6875%\n",
      "Train accuracy at Epoch  99:\t\t85.1214%\n",
      "Validation accuracy at Epoch  99:\t78.0125%\n",
      "Train accuracy at Epoch 100:\t\t86.7405%\n",
      "Validation accuracy at Epoch 100:\t73.1000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy at Epoch 101:\t\t87.1000%\n",
      "Validation accuracy at Epoch 101:\t78.9500%\n",
      "Train accuracy at Epoch 102:\t\t86.2452%\n",
      "Validation accuracy at Epoch 102:\t77.0625%\n",
      "Train accuracy at Epoch 103:\t\t86.7000%\n",
      "Validation accuracy at Epoch 103:\t75.5625%\n",
      "Train accuracy at Epoch 104:\t\t84.3286%\n",
      "Validation accuracy at Epoch 104:\t73.3250%\n",
      "Train accuracy at Epoch 105:\t\t84.4286%\n",
      "Validation accuracy at Epoch 105:\t72.3000%\n",
      "Train accuracy at Epoch 106:\t\t86.7905%\n",
      "Validation accuracy at Epoch 106:\t75.8875%\n",
      "Train accuracy at Epoch 107:\t\t87.4119%\n",
      "Validation accuracy at Epoch 107:\t77.4375%\n",
      "Train accuracy at Epoch 108:\t\t86.3405%\n",
      "Validation accuracy at Epoch 108:\t76.7875%\n",
      "Train accuracy at Epoch 109:\t\t87.3048%\n",
      "Validation accuracy at Epoch 109:\t77.7750%\n",
      "Train accuracy at Epoch 110:\t\t87.7667%\n",
      "Validation accuracy at Epoch 110:\t78.8125%\n",
      "Train accuracy at Epoch 111:\t\t84.5881%\n",
      "Validation accuracy at Epoch 111:\t77.6000%\n",
      "Train accuracy at Epoch 112:\t\t87.4714%\n",
      "Validation accuracy at Epoch 112:\t78.3000%\n",
      "Train accuracy at Epoch 113:\t\t87.5690%\n",
      "Validation accuracy at Epoch 113:\t76.3875%\n",
      "Train accuracy at Epoch 114:\t\t87.3905%\n",
      "Validation accuracy at Epoch 114:\t78.7000%\n",
      "Train accuracy at Epoch 115:\t\t85.3190%\n",
      "Validation accuracy at Epoch 115:\t77.9000%\n",
      "Train accuracy at Epoch 116:\t\t82.4190%\n",
      "Validation accuracy at Epoch 116:\t77.5875%\n",
      "Train accuracy at Epoch 117:\t\t86.7857%\n",
      "Validation accuracy at Epoch 117:\t76.3750%\n",
      "Train accuracy at Epoch 118:\t\t87.1524%\n",
      "Validation accuracy at Epoch 118:\t78.1500%\n",
      "Train accuracy at Epoch 119:\t\t86.7881%\n",
      "Validation accuracy at Epoch 119:\t78.6625%\n",
      "Train accuracy at Epoch 120:\t\t87.8214%\n",
      "Validation accuracy at Epoch 120:\t76.0375%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch):  \n",
    "    # train the model using train set\n",
    "    net.train()\n",
    "    \n",
    "    num_correct_train = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # train your model\n",
    "        ############################################################\n",
    "        ############## Add Your Code Here ##########################\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = torch.argmax(outputs.detach(), dim=1)\n",
    "        num_correct_train += torch.sum(out == labels).item()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ############################################################\n",
    "\n",
    "    # print the accuracy of the train batch\n",
    "    accuracy_train = num_correct_train / len(trainset) * 100\n",
    "    print(f\"Train accuracy at Epoch {epoch+1:3d}:\\t\\t{accuracy_train:0.4f}%\")\n",
    "        \n",
    "    # validate the model performance before testing\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        num_correct_val = 0\n",
    "        \n",
    "        for inputs, labels in valloader:       \n",
    "        ############################################################\n",
    "        ############## Add Your Code Here ##########################        \n",
    " \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            out = net(inputs)\n",
    "\n",
    "            out = torch.argmax(out, dim=1)\n",
    "            num_correct_val += torch.sum(out == labels).item()\n",
    "        ############################################################\n",
    "        # print the accuracy of the whole validation data\n",
    "        accuracy_val = num_correct_val / len(valset) * 100\n",
    "        print(f\"Validation accuracy at Epoch {epoch+1:3d}:\\t{accuracy_val:0.4f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a13eae-5411-4d9a-b743-1f049d48fc8f",
   "metadata": {},
   "source": [
    "## Test your model at the real testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06f57d1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "v6fzf0qNOqV9",
    "outputId": "c76a009c-838c-4b49-8888-220299ae8fdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 79.6200%\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "num_correct_test = 0\n",
    "with torch.no_grad():\n",
    "    for inputs,labels in testloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        out = net(inputs)\n",
    "        out = torch.argmax(out, dim=1)  \n",
    "        num_correct_test += torch.sum(out==labels).item()\n",
    "print(f\"Test accuracy: {num_correct_test*100/len(testset):0.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493c16bd-3fe6-41c9-b699-bf702a23cae7",
   "metadata": {},
   "source": [
    "## Grading Critera\n",
    "1. (10 pts) The submission contains both HTML and IPYNB with the same cell ouputs.\n",
    "2. (30 pts) The whole IPYNB can be executed without any errors.\n",
    "3. (10 pts) The CNN model is correctly implemented.\n",
    "4. (10 pts) The loss function and optimizer are correctly implemeted\n",
    "5. (20 pts) The training part is correctly implemented.\n",
    "6. (10 pts) The validation part is correctly implemented.\n",
    "7. (10 pts) The accuracy is larger than 70% (with regards to the test set) through parameter tuning (learning rate, batch size, optimizer, model structure, transform)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
